{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7d3a52",
   "metadata": {},
   "source": [
    "# üìå Embedding Text and Visualizing in 3D\n",
    "\n",
    "## What is an Embedding?\n",
    "\n",
    "An **embedding** is a way to convert text (or any data like images, audio) into numbers‚Äîspecifically, a vector of floating-point numbers. These vectors capture the **meaning** or **semantics** of the text.\n",
    "\n",
    "For example, the sentences:\n",
    "- ‚ÄúI love pizza.‚Äù\n",
    "- ‚ÄúPizza is my favorite food.‚Äù\n",
    "\n",
    "...may have different words but very **similar meanings**, so their embeddings will be **close together** in vector space.\n",
    "\n",
    "---\n",
    "\n",
    "## Why are Embeddings Important?\n",
    "\n",
    "Text data is not naturally understandable by machines. Computers understand numbers. Embeddings allow us to:\n",
    "\n",
    "- Convert unstructured text into structured numerical data.\n",
    "- Compare the similarity between pieces of text.\n",
    "- Use math to cluster, search, or classify text.\n",
    "\n",
    "---\n",
    "\n",
    "## How Are Embeddings Used in LLMs and Vector Databases?\n",
    "\n",
    "### üîπ In LLMs (Large Language Models)\n",
    "LLMs like GPT or BERT use embeddings internally:\n",
    "- They convert words/sentences into embeddings as the **input**.\n",
    "- Then, they process these embeddings to generate new text.\n",
    "- They also produce output embeddings for the generated tokens.\n",
    "\n",
    "Embeddings help LLMs **understand context, meaning, and relationships between words**.\n",
    "\n",
    "### üîπ In Vector Databases (e.g., FAISS, Milvus, Pinecone)\n",
    "Vector databases store embeddings and let you:\n",
    "- Search for the most similar vectors (text, images, etc.)\n",
    "- Perform semantic search: **‚ÄúFind me texts similar in meaning to this.‚Äù**\n",
    "- Power RAG (Retrieval-Augmented Generation) systems.\n",
    "\n",
    "---\n",
    "\n",
    "## What You‚Äôll Do in This Notebook\n",
    "\n",
    "We will:\n",
    "- Use the `sentence-transformers` library to embed some example texts.\n",
    "- Use **PCA** to reduce the embedding dimensions from 384D to 3D (for visualization).\n",
    "- Plot the 3D embeddings with `matplotlib` to **see how meaning is captured visually**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fb7e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install sentence-transformers matplotlib scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f80e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca24e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text data\n",
    "texts = [\n",
    "    \"Artificial intelligence is the future of technology.\",\n",
    "    \"Machine learning improves applications using data.\",\n",
    "    \"I love pizza on weekends!\",\n",
    "    \"The universe is vast and mysterious.\",\n",
    "    \"Natural language processing deals with text and language.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e337d58",
   "metadata": {},
   "source": [
    "## Step 1: Converting Text to Embeddings\n",
    "\n",
    "- We'll use a **pre-trained model** (`all-MiniLM-L6-v2`) from the Sentence Transformers library.\n",
    "- This model turns each sentence into a **384-dimensional vector** that captures the sentence's meaning.\n",
    "- So if you have `N` sentences, you'll get an Nx384 matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fae5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained embedding model.\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert texts to embeddings.\n",
    "embeddings = model.encode(texts)\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "print(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340bbaa",
   "metadata": {},
   "source": [
    "### Step 2: Reducing Embeddings to 3D for Visualization\n",
    "\n",
    "- Each embedding is a **384-dimensional vector** ‚Äî impossible to visualize directly.\n",
    "- We use **PCA (Principal Component Analysis)** to reduce each vector to **3 dimensions**.\n",
    "- This allows us to **plot them in 3D space**, while preserving as much information as possible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb1cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "print(\"Reduced shape:\\n\\n\", reduced_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62b912",
   "metadata": {},
   "source": [
    "### Step 3: Plotting the 3D Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45120333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 3D embeddings.\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Scatter plot of the reduced embeddings.\n",
    "x, y, z = reduced_embeddings[:, 0], reduced_embeddings[:, 1], reduced_embeddings[:, 2]\n",
    "ax.scatter(x, y, z, c='blue', s=60)\n",
    "\n",
    "print(\"X :\", x)\n",
    "print(\"Y :\", y)\n",
    "print(\"Z :\", z)\n",
    "\n",
    "# Annotate points with text index\n",
    "for i, txt in enumerate(texts):\n",
    "    ax.text(x[i], y[i], z[i], f\"{i+1}\", fontsize=10)\n",
    "\n",
    "ax.set_title(\"3D Visualization of Text Embeddings\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e51bd",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary: What Did We Learn?\n",
    "\n",
    "- We took natural language text and turned it into meaningful numerical vectors (embeddings).\n",
    "- These embeddings let us **measure semantic similarity** between texts.\n",
    "- We reduced high-dimensional data (384D) into 3D using PCA for visualization.\n",
    "- Embeddings are the **foundation of LLMs, semantic search, and AI applications** like chatbots, search engines, and recommendation systems.\n",
    "\n",
    "üëâ If you understand embeddings, you're one step closer to understanding how AI truly \"understands\" language!\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
